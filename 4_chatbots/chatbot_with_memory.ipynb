{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "store = {}\n",
    "def get_session_history(session_id:str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "llm = ChatGroq(model=\"gemma2-9b-it\",groq_api_key=groq_api_key,)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful assistant please answer the question.\"),\n",
    "    (\"human\",\"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm \n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    ")\n",
    "\n",
    "config = {\"configurable\":{\"session_id\":\"1\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"input\": \"who is narendra mode ?\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"input\": \"what question i asked you previously ?\",\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"input\": \"summerize the conversaction \",\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': InMemoryChatMessageHistory(messages=[HumanMessage(content='who is narendra mode ?'), AIMessage(content=\"Narendra Modi is the current Prime Minister of India. \\n\\nHere are some key facts about him:\\n\\n* **Full Name:** Narendra Damodardas Modi\\n* **Born:** September 17, 1950, in Vadnagar, Gujarat, India\\n* **Political Party:** Bharatiya Janata Party (BJP)\\n* **Prime Minister of India:** Since May 26, 2014\\n* **Background:** Modi rose through the ranks of the BJP, serving as Chief Minister of Gujarat from 2001 to 2014. He is known for his strong leadership, economic reforms, and focus on national security.\\n\\nIf you'd like to know more about a specific aspect of his life or career, just ask!  \\n\\n\", response_metadata={'token_usage': {'completion_tokens': 166, 'prompt_tokens': 32, 'total_tokens': 198, 'completion_time': 0.301818182, 'prompt_time': 0.000333969, 'queue_time': 0.01300888, 'total_time': 0.302152151}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-b19dac8e-08e5-42a9-97cc-a34eac25ec4c-0', usage_metadata={'input_tokens': 32, 'output_tokens': 166, 'total_tokens': 198}), HumanMessage(content='what question i asked you previously ?'), AIMessage(content='You asked: \"who is narendra mode ?\" \\n', response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 450, 'total_tokens': 464, 'completion_time': 0.025454545, 'prompt_time': 0.018374054, 'queue_time': 0.0028180839999999985, 'total_time': 0.043828599}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-159eebd4-7867-4bca-8c6d-932d4923fd26-0', usage_metadata={'input_tokens': 450, 'output_tokens': 14, 'total_tokens': 464}), HumanMessage(content='summerize the conversaction '), AIMessage(content='You asked who Narendra Modi is. I provided a summary of his key facts, including his full name, birthdate, political party, role as Prime Minister of India, and background. You then asked what your previous question was, and I confirmed it was about Narendra Modi. \\n\\n\\nLet me know if you have any other questions! \\n', response_metadata={'token_usage': {'completion_tokens': 71, 'prompt_tokens': 717, 'total_tokens': 788, 'completion_time': 0.129090909, 'prompt_time': 0.023067048, 'queue_time': 0.0029316389999999998, 'total_time': 0.152157957}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-9aa14fa7-b734-4e04-876d-075159ce8302-0', usage_metadata={'input_tokens': 717, 'output_tokens': 71, 'total_tokens': 788})])}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.messages import AIMessage,  HumanMessage\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "def get_session_history(session_id:str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"gemma2-9b-it\",groq_api_key=groq_api_key,)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful assistant please answer the question.\"),\n",
    "    (\"human\",\"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm \n",
    "# chain.invoke({\"messages\":[\n",
    "#     HumanMessage(content=\"Hii, My name is spandan\"),\n",
    "# ],\"input\":\"who am i ?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    ")\n",
    "\n",
    "config = {\"configurable\":{\"session_id\":\"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"input\": \"who is narendra mode ?\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"input\": \"what question i asked you previously ?\",\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='You asked: \"who is narendra mode ?\" \\n' response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 438, 'total_tokens': 452, 'completion_time': 0.025454545, 'prompt_time': 0.013814287, 'queue_time': 0.002980021000000001, 'total_time': 0.039268832}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-c038e24f-b6b9-4322-8199-0adbde137520-0' usage_metadata={'input_tokens': 438, 'output_tokens': 14, 'total_tokens': 452}\n",
      "{'1': InMemoryChatMessageHistory(messages=[HumanMessage(content='who is narendra mode ?'), AIMessage(content=\"Narendra Modi is the current Prime Minister of India. \\n\\nHere are some key facts about him:\\n\\n* **Full Name:** Narendra Damodardas Modi\\n* **Born:** September 17, 1950, in Vadnagar, Gujarat, India\\n* **Political Party:** Bharatiya Janata Party (BJP)\\n* **Prime Minister of India:** Since May 26, 2014\\n\\nNarendra Modi is a prominent figure in Indian politics known for his strong leadership and right-wing nationalist ideology. \\n\\nIf you'd like to know more about a specific aspect of his life or career, please ask! \\n\", response_metadata={'token_usage': {'completion_tokens': 140, 'prompt_tokens': 32, 'total_tokens': 172, 'completion_time': 0.254545455, 'prompt_time': 0.000358418, 'queue_time': 0.014861781000000001, 'total_time': 0.254903873}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-117d3c47-6b41-4ea7-a6d5-0bd0114615c8-0', usage_metadata={'input_tokens': 32, 'output_tokens': 140, 'total_tokens': 172}), HumanMessage(content='what question i asked you previously ?'), AIMessage(content='You asked: \"who is narendra mode ?\" \\n', response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 438, 'total_tokens': 452, 'completion_time': 0.025454545, 'prompt_time': 0.013814287, 'queue_time': 0.002980021000000001, 'total_time': 0.039268832}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-c038e24f-b6b9-4322-8199-0adbde137520-0', usage_metadata={'input_tokens': 438, 'output_tokens': 14, 'total_tokens': 452})])}\n"
     ]
    }
   ],
   "source": [
    "print(response)\n",
    "print(store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store conversational history in vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"conv_history\",\n",
    "    embedding_function=embedding\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "# Modify get_session_history to use Chroma instead of store\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    # Perform similarity search using session ID as the query\n",
    "    results = vector_store.similarity_search_with_score(session_id)\n",
    "\n",
    "    # If no relevant result is found (empty history), return an empty ChatMessageHistory\n",
    "    if not results:  # No match or low similarity score\n",
    "        return ChatMessageHistory()  # Return empty history if no match\n",
    "    \n",
    "    # Deserialize the retrieved stored history\n",
    "    history_data = []\n",
    "    for result in results[-3:]:\n",
    "        history_data.append(eval(result[0].page_content)[\"messages\"])\n",
    "\n",
    "    # Create a new ChatMessageHistory instance from the saved history data\n",
    "    history = ChatMessageHistory()\n",
    "    history.messages = history_data  # Assuming messages are stored in a dict format\n",
    "    return history  # Return the populated history\n",
    "\n",
    "def save_session_history(session_id: str, history: ChatMessageHistory):\n",
    "    # Manually construct the history as a dictionary\n",
    "    history_data = {\n",
    "        \"messages\":  history.messages\n",
    "    }\n",
    "\n",
    "    # Create a Document object to store in the vector store\n",
    "    document = Document(\n",
    "        page_content=str(history_data),  # Store as stringified dictionary\n",
    "        metadata={\"session_id\": session_id}  # Attach session ID as metadata\n",
    "    )\n",
    "\n",
    "    # Add the document to Chroma (embed and store)\n",
    "    vector_store.add_documents([document])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm = ChatGroq(model=\"gemma2-9b-it\",groq_api_key=groq_api_key,)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful assistant please answer the question.\"),\n",
    "    (\"human\",\"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm \n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    ")\n",
    "\n",
    "config = {\"configurable\":{\"session_id\":\"1\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"input\": \"who is narendra mode ?\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Get session history after first interaction and save it\n",
    "history = get_session_history(\"1\")\n",
    "history.add_user_message(\"who is narendra mode?\")  # Add the user message\n",
    "history.add_ai_message(response)               # Add the assistant's response\n",
    "save_session_history(\"1\", history)                   # Save history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"input\": \"what is narendra modi mothers name ?\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Get session history after first interaction and save it\n",
    "history = get_session_history(\"1\")\n",
    "history.add_user_message(\"what is narendra modi mothers name ?\")  # Add the user message\n",
    "history.add_ai_message(response)               # Add the assistant's response\n",
    "save_session_history(\"1\", history)                   # Save history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[[HumanMessage(content='what is narendra modi mothers name ?'), AIMessage(content=\"Narendra Modi's mother's name was **Heeraben Modi**. \\n\", response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 34, 'total_tokens': 54, 'completion_time': 0.036363636, 'prompt_time': 0.000402909, 'queue_time': 0.01350043, 'total_time': 0.036766545}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-cffccb7e-3bb5-497b-9994-321a9fd8b03c-0', usage_metadata={'input_tokens': 34, 'output_tokens': 20, 'total_tokens': 54})], [HumanMessage(content='who is narendra mode?'), AIMessage(content=\"Narendra Modi is the current Prime Minister of India.  \\n\\nHere are some key facts about him:\\n\\n* **Full Name:** Narendra Damodardas Modi\\n* **Born:** September 17, 1950, in Vadnagar, Gujarat, India\\n* **Political Party:** Bharatiya Janata Party (BJP)\\n* **Prime Minister of India:** Since May 26, 2014\\n* **Background:** Modi rose through the ranks of the BJP, serving as Chief Minister of Gujarat from 2001 to 2014. He is known for his economic policies and his focus on national security.\\n\\nWould you like to know more about a specific aspect of Narendra Modi's life or career?  \\n\\n\", response_metadata={'token_usage': {'completion_tokens': 161, 'prompt_tokens': 32, 'total_tokens': 193, 'completion_time': 0.292727273, 'prompt_time': 0.000410189, 'queue_time': 0.01283066, 'total_time': 0.293137462}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-d4759f9e-1a6a-4c12-9a89-dc67a641d2b0-0', usage_metadata={'input_tokens': 32, 'output_tokens': 161, 'total_tokens': 193})]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_session_history(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='what is narendra modi mothers name ?'), AIMessage(content=\"Narendra Modi's mother's name was **Heeraben Modi**. \\n\", response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 34, 'total_tokens': 54, 'completion_time': 0.036363636, 'prompt_time': 0.000402909, 'queue_time': 0.01350043, 'total_time': 0.036766545}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-cffccb7e-3bb5-497b-9994-321a9fd8b03c-0', usage_metadata={'input_tokens': 34, 'output_tokens': 20, 'total_tokens': 54})]\n",
      "[HumanMessage(content='who is narendra mode?'), AIMessage(content=\"Narendra Modi is the current Prime Minister of India.  \\n\\nHere are some key facts about him:\\n\\n* **Full Name:** Narendra Damodardas Modi\\n* **Born:** September 17, 1950, in Vadnagar, Gujarat, India\\n* **Political Party:** Bharatiya Janata Party (BJP)\\n* **Prime Minister of India:** Since May 26, 2014\\n* **Background:** Modi rose through the ranks of the BJP, serving as Chief Minister of Gujarat from 2001 to 2014. He is known for his economic policies and his focus on national security.\\n\\nWould you like to know more about a specific aspect of Narendra Modi's life or career?  \\n\\n\", response_metadata={'token_usage': {'completion_tokens': 161, 'prompt_tokens': 32, 'total_tokens': 193, 'completion_time': 0.292727273, 'prompt_time': 0.000410189, 'queue_time': 0.01283066, 'total_time': 0.293137462}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-d4759f9e-1a6a-4c12-9a89-dc67a641d2b0-0', usage_metadata={'input_tokens': 32, 'output_tokens': 161, 'total_tokens': 193})]\n"
     ]
    }
   ],
   "source": [
    "res = vector_store.similarity_search_with_score(\"1\")\n",
    "for response in res:\n",
    "    print(eval(response[0].page_content)[\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"input\": \"how to optimize openai token using langchain?\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "history = get_session_history(\"1\")\n",
    "history.add_user_message(\"how to optimize openai token using langchain?\")  # Add the user message\n",
    "history.add_ai_message(response)               # Add the assistant's response\n",
    "save_session_history(\"1\", history) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Let\\'s talk about optimizing OpenAI token usage with LangChain! \\n\\n**Understanding the Challenge**\\n\\nOpenAI\\'s models, like GPT-3, are powerful but have a limit on the number of tokens they can process in a single request. Tokens are like the building blocks of text, and exceeding this limit results in errors or truncated responses.\\n\\n**LangChain to the Rescue**\\n\\nLangChain is a fantastic framework for building applications with large language models (LLMs) like OpenAI\\'s. It offers several features that directly address token optimization:\\n\\n1. **Chunking:**\\n\\n   - **The Problem:**  Long pieces of text are difficult for LLMs to handle efficiently. \\n   - **The Solution:** LangChain lets you break down large inputs into smaller, manageable chunks. You send each chunk to the LLM separately, and then piece the responses back together.\\n\\n2. **Memory:**\\n\\n   - **The Problem:** LLMs have limited short-term memory. They \"forget\" previous parts of a conversation quickly.\\n   - **The Solution:** LangChain provides memory mechanisms (e.g., using a `ConversationBufferMemory`) to store context from earlier interactions. This allows your LLM to maintain a coherent understanding of the ongoing conversation, reducing the need to repeat information.\\n\\n3. **Prompt Engineering:**\\n\\n   - **The Problem:** Poorly structured prompts can lead to LLMs generating verbose or irrelevant responses, wasting tokens.\\n   - **The Solution:** LangChain offers tools and techniques for crafting effective prompts that are clear, concise, and guide the LLM towards the desired output.\\n\\n4. **Tools and Agents:**\\n\\n   - **The Problem:** Sometimes, you need to perform actions beyond pure text generation (e.g., fetching data from a database).\\n   - **The Solution:** LangChain\\'s \"agents\" can integrate with external tools. This allows your LLM to access and process information from the real world, potentially reducing the need for it to generate everything from scratch.\\n\\n**Example: Chatbot with Chunking and Memory**\\n\\n```python\\nfrom langchain.memory import ConversationBufferMemory\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains import ConversationChain\\n\\nllm = OpenAI(temperature=0)\\nmemory = ConversationBufferMemory()\\nchat = ConversationChain(llm=llm, memory=memory)\\n\\nwhile True:\\n    user_input = input(\"You: \")\\n    response = chat.run(user_input)\\n    print(\"Bot:\", response)\\n```\\n\\n**Key Points:**\\n\\n- Experiment with different chunking sizes to find the optimal balance between reducing token usage and maintaining context.\\n- Consider the type of application you\\'re building. Memory management and prompt engineering techniques will vary depending on the task.\\n- LangChain\\'s documentation is excellent: [https://python.langchain.com/](https://python.langchain.com/)\\n\\n\\n\\nLet me know if you have any more questions or want to dive into specific optimization strategies!\\n', response_metadata={'token_usage': {'completion_tokens': 635, 'prompt_tokens': 1719, 'total_tokens': 2354, 'completion_time': 1.154545455, 'prompt_time': 0.057434428, 'queue_time': 0.002921251999999999, 'total_time': 1.211979883}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-99d372b0-6668-4fc4-916c-9aca2f02d7b5-0', usage_metadata={'input_tokens': 1719, 'output_tokens': 635, 'total_tokens': 2354})"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You\\'re right to ask! There are indeed several other ways to optimize OpenAI token usage with LangChain beyond the ones I mentioned. \\n\\nHere are a few more strategies:\\n\\n* **Parameter Tuning:** Experiment with different OpenAI API parameters like `max_tokens` and `temperature`.  \\n    * `max_tokens`: Controls the maximum number of tokens OpenAI will generate in a response.  Adjust this to fit your needs.\\n    * `temperature`:  Influences the \"creativity\" of the LLM\\'s output. A lower temperature produces more deterministic, focused responses, which can be more token-efficient.\\n\\n* **Quantization:** Use quantized models when available. Quantization reduces the precision of the model\\'s weights, making it smaller and faster, often with minimal impact on performance. LangChain might have built-in support for this or you might need to explore OpenAI\\'s API options.\\n\\n* **Fine-Tuning:** If you have a specific use case, consider fine-tuning a smaller OpenAI model on your own dataset. This can lead to more specialized and efficient performance for your particular task.\\n\\n* **Tokenizers:**  Explore different tokenizers. While OpenAI provides its own tokenizer, there might be other tokenizers (like SentencePiece) that could be more efficient for your type of text.\\n\\n* **Compression Techniques:** Research text compression techniques like  Brotli or Zstandard. You could compress your input text before sending it to the LLM and then decompress the output, potentially saving tokens.\\n\\n**Important Considerations:**\\n\\n* **Trade-offs:** Token optimization often involves trade-offs. For instance, using a smaller model or lower temperature might sacrifice some accuracy or creativity.\\n* **Experimentation:** The best optimization strategy depends heavily on your specific use case and the type of text you\\'re working with.  Don\\'t be afraid to experiment and benchmark different approaches to find what works best for you. \\n\\n\\n\\nLet me know if you\\'d like to explore any of these strategies in more detail!\\n', response_metadata={'token_usage': {'completion_tokens': 428, 'prompt_tokens': 2670, 'total_tokens': 3098, 'completion_time': 0.778181818, 'prompt_time': 0.086507805, 'queue_time': 0.0033706860000000116, 'total_time': 0.864689623}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-0f528f77-9c36-4ae7-be20-f3dd8a073fb3-0', usage_metadata={'input_tokens': 2670, 'output_tokens': 428, 'total_tokens': 3098})"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\n",
    "        \"input\": \"is there any other way ?\",\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
