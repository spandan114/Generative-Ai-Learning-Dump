{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()   \n",
    "os.environ[\"HF_TOKEN\"] = os.getenv(\"HF_TOKEN\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import PineconeHybridSearchRetriever\n",
    "from pinecone import ServerlessSpec,Pinecone\n",
    "index_name = \"hybrid-search-langchain\"\n",
    "pc = Pinecone(pinecone_api_key=pinecone_api_key)\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric=\"dotproduct\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\",region=\"us-east-1\"),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone.data.index.Index at 0x105e5f9d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pc.Index(index_name)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/opt/anaconda3/envs/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone_text.sparse import BM25Encoder\n",
    "\n",
    "bm25 = BM25Encoder().default()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 164.26it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "\"The langchain_huggingface library simplifies natural language processing tasks. It integrates Hugging Face's transformers with Langchain's functionality. This combination enables efficient embedding, classification, and generation. Developers can leverage pre-trained models for various applications. The library supports multiple models and architectures. Its flexibility accelerates NLP development.\",\n",
    "\"Hugging Face's transformers revolutionized NLP capabilities. These models learn contextual relationships within text. Pre-trained models like BERT and MiniLM facilitate downstream tasks. Fine-tuning enables adaptation to specific domains. Transformers' attention mechanism improves upon traditional recurrent neural networks. This architecture boosts performance in language understanding and generation.\",\n",
    "\"NLP applications require high-quality embeddings. Sentence transformers provide dense vector representations. These embeddings capture semantic meaning and context. They enable semantic search, clustering, and text classification. Efficient computation and compact representations make them suitable for large-scale applications. Sentence transformers empower developers to build robust NLP systems.\",\n",
    "\"The 'all-MiniLM-L6-v2' model excels at sentence embeddings. Its compact size ensures efficient computation. This model's performance rivals larger counterparts. Its suitability for various NLP tasks makes it a popular choice. Developers leverage this model for semantic search, text classification, and clustering. Its reliability enhances downstream applications.\",\n",
    "\"Langchain_huggingface streamlines embedding generation. It abstracts underlying complexities, allowing developers to focus on application logic. This library supports batch processing and multiple input formats. Its flexibility accommodates diverse use cases. By integrating Hugging Face's transformers, langchain_huggingface accelerates NLP development. Efficient embedding generation empowers innovative applications.\",\n",
    "\"Developers can explore alternative models within langchain_huggingface. Each model offers unique strengths and trade-offs. 'bert-base-nli-mean-tokens' and 'distilbert-base-nli-mean-tokens' provide additional options. These models cater to specific requirements. By selecting the optimal model, developers can fine-tune their NLP applications for enhanced performance and accuracy.\"\n",
    "]\n",
    "\n",
    "bm25.fit(sentences)\n",
    "\n",
    "bm25.dump(\"bm25_val.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_encoder = BM25Encoder().load(\"bm25_val.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = PineconeHybridSearchRetriever(\n",
    "    embeddings=embedding,\n",
    "    sparse_encoder=bm25_encoder,\n",
    "    index=index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.82s/it]\n"
     ]
    }
   ],
   "source": [
    "retriever.add_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'score': 0.5075863}, page_content=\"The langchain_huggingface library simplifies natural language processing tasks. It integrates Hugging Face's transformers with Langchain's functionality. This combination enables efficient embedding, classification, and generation. Developers can leverage pre-trained models for various applications. The library supports multiple models and architectures. Its flexibility accelerates NLP development.\"),\n",
       " Document(metadata={'score': 0.372083366}, page_content=\"Langchain_huggingface streamlines embedding generation. It abstracts underlying complexities, allowing developers to focus on application logic. This library supports batch processing and multiple input formats. Its flexibility accommodates diverse use cases. By integrating Hugging Face's transformers, langchain_huggingface accelerates NLP development. Efficient embedding generation empowers innovative applications.\"),\n",
       " Document(metadata={'score': 0.305434108}, page_content=\"Developers can explore alternative models within langchain_huggingface. Each model offers unique strengths and trade-offs. 'bert-base-nli-mean-tokens' and 'distilbert-base-nli-mean-tokens' provide additional options. These models cater to specific requirements. By selecting the optimal model, developers can fine-tune their NLP applications for enhanced performance and accuracy.\"),\n",
       " Document(metadata={'score': 0.162960649}, page_content=\"Hugging Face's transformers revolutionized NLP capabilities. These models learn contextual relationships within text. Pre-trained models like BERT and MiniLM facilitate downstream tasks. Fine-tuning enables adaptation to specific domains. Transformers' attention mechanism improves upon traditional recurrent neural networks. This architecture boosts performance in language understanding and generation.\")]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"primary function of the langchain_huggingface library\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
